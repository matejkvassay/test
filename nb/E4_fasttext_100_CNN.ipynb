{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import normalize as scikit_normalize\n",
    "from evaluation import plot_history\n",
    "from evaluation import rmse_report\n",
    "from sampling import UnderSampler3D\n",
    "from fasttext_embedding import FastTextEmbeddingBag\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM_CHECKPOINT=True\n",
    "\n",
    "DATASET='/home/kvassay/data/z/data/reviews_train_test_dev1_{}.pickle'\n",
    "TYPE='tok'\n",
    "TB_LOG_DIR='/home/kvassay/data/z/log/E4/scalars/'\n",
    "VEC_DIM=100\n",
    "FASTTEXT='/home/kvassay/data/z/models/fasttext/cbow_{}_e{}_w{}.bin'.format(VEC_DIM,50,5)\n",
    "SEQ_PADDING=50\n",
    "CHECKPOINT_DIR='/tmp/z/checkpoint_dim{}_pad{}/'.format(VEC_DIM,SEQ_PADDING)\n",
    "ALLOWED_SPECIAL=tuple(['?','!',':(', ':)', ':D',':-)',':-D',':\\'(',':/',':-/','<3',':-P',':P'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.69 s, sys: 1.11 s, total: 5.8 s\n",
      "Wall time: 5.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(DATASET.format(TYPE),'rb') as f:\n",
    "    train,dev,_=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not FROM_CHECKPOINT:\n",
    "    fasttext=FastTextEmbeddingBag(FASTTEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text + extract features\n",
    "- filter out EN stop-words (and, or, ...)\n",
    "- filter out non-allowed special tokens (we want to keep smileys and !,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_special= re.compile(\"|\".join(re.escape(s) for s in ALLOWED_SPECIAL))\n",
    "\n",
    "def word_filter(word):\n",
    "    if word in STOP_WORDS:\n",
    "        return False\n",
    "    if not word.isalpha():\n",
    "        if not rx_special.findall(word):\n",
    "            return False\n",
    "    else:\n",
    "        if len(word)<3:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return [x.lower() for x in text if word_filter(x.lower())]\n",
    "\n",
    "def preprocess_texts(dataset,text_keys=['summary','text']):\n",
    "    for sample in tqdm(dataset):\n",
    "        for key in text_keys:\n",
    "            sample[key]=preprocess_text(sample[key])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 6 µs, total: 6 µs\n",
      "Wall time: 10.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not FROM_CHECKPOINT:\n",
    "    train=preprocess_texts(train)\n",
    "    dev=preprocess_texts(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "- transform texts to averages of their fastText vectors\n",
    "- concatenate summary & text average vectors into single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_vecs(vecs_mtx,length):\n",
    "    return pad_sequences(vecs_mtx,\n",
    "                         maxlen=length,\n",
    "                         dtype='float32',\n",
    "                        padding='post',\n",
    "                        truncating='post')\n",
    "\n",
    "def extract_features(dataset, fasttext):\n",
    "    default_vec=np.zeros(VEC_DIM,dtype=np.float32)\n",
    "    vecs_all=[]\n",
    "    for sample in tqdm(dataset):\n",
    "        all_words=sample['summary']+sample['text']\n",
    "        if all_words:\n",
    "            vecs=fasttext.forward([x for x in all_words])\n",
    "        else:\n",
    "            vecs=np.array([default_vec])\n",
    "        vecs=scikit_normalize(vecs)\n",
    "        vecs=vecs.reshape(1,vecs.shape[0],vecs.shape[1])         \n",
    "        vecs = pad_vecs(vecs, SEQ_PADDING)\n",
    "        vecs_all.append(vecs)\n",
    "    vecs_all=np.array(vecs_all)\n",
    "    vecs_all=vecs_all.reshape(vecs_all.shape[0],vecs_all.shape[2],vecs_all.shape[3])\n",
    "    return vecs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint():\n",
    "    with open(CHECKPOINT_DIR+'X_train.npy','rb') as f:\n",
    "        X_train=np.load(f)\n",
    "    with open(CHECKPOINT_DIR+'X_dev.npy','rb') as f:\n",
    "        X_dev=np.load(f)\n",
    "    with open(CHECKPOINT_DIR+'y_train.npy','rb') as f:\n",
    "        y_train=np.load(f)\n",
    "    with open(CHECKPOINT_DIR+'y_dev.npy','rb') as f:\n",
    "        y_dev=np.load(f)\n",
    "    return X_train,X_dev,y_train,y_dev\n",
    "        \n",
    "def checkpoint(X_train,X_dev,y_train,y_dev):\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        os.makedirs(CHECKPOINT_DIR)\n",
    "    with open(CHECKPOINT_DIR+'X_train.npy','wb') as f:\n",
    "        np.save(f,X_train)\n",
    "    with open(CHECKPOINT_DIR+'X_dev.npy','wb') as f:\n",
    "        np.save(f,X_dev)\n",
    "    with open(CHECKPOINT_DIR+'y_train.npy','wb') as f:\n",
    "        np.save(f,y_train)\n",
    "    with open(CHECKPOINT_DIR+'y_dev.npy','wb') as f:\n",
    "        np.save(f,y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 8.27 s, total: 8.41 s\n",
      "Wall time: 54.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not FROM_CHECKPOINT:\n",
    "    X_train=extract_features(train,fasttext)\n",
    "    X_dev=extract_features(dev,fasttext)\n",
    "    y_train=np.array([x['score'] for x in train])\n",
    "    y_dev=np.array([x['score'] for x in dev])\n",
    "    print('Train samples shape: {}, Dev samples shape: {}'.format(X_train.shape,X_dev.shape))\n",
    "else:\n",
    "    X_train,X_dev,y_train,y_dev=load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 2 µs, total: 2 µs\n",
      "Wall time: 3.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not FROM_CHECKPOINT:\n",
    "    checkpoint(X_train,X_dev,y_train,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(learning_rate,epochs,batch_size,name,steps):\n",
    "    model=train_model(epochs=epochs,batch_size=batch_size,learning_rate=learning_rate,steps=steps)\n",
    "    y_pred_dev=model.predict(X_dev)\n",
    "    rmse_report(y_dev,y_pred_dev,title='{} - RMSE report'.format(name))\n",
    "    plot_history(model,title='{} - Train/Dev MSE'.format(name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tb_callback():\n",
    "    suffix=datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir= os.path.join(TB_LOG_DIR,suffix)\n",
    "    return K.callbacks.TensorBoard(log_dir=os.path.join(log_dir))\n",
    "\n",
    "def train_model(batch_size,learning_rate, epochs,steps):\n",
    "    DROPRATE=0.1\n",
    "    tensorboard_callback = get_tb_callback()\n",
    "    model = K.models.Sequential([\n",
    "        K.layers.Conv1D(256, 10, activation='relu',strides=2,padding='same',\n",
    "                        input_shape=(X_train.shape[1],X_train.shape[2])),\n",
    "        K.layers.GlobalMaxPooling1D(),\n",
    "        K.layers.Dense(256,activation='relu'),\n",
    "        K.layers.Dense(1,activation='linear')])\n",
    "    opt=K.optimizers.Adam(lr=learning_rate, decay=learning_rate/epochs)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    sampler=UnderSampler3D(X_train,y_train,batch_size=batch_size)\n",
    "    model.fit_generator(sampler,\n",
    "                        shuffle=False,\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch=steps,\n",
    "                        validation_data=(X_dev,y_dev),\n",
    "                        callbacks=[tensorboard_callback])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "300/300 [==============================] - 5s 18ms/step - loss: 28.1245 - val_loss: 1.0767\n",
      "Epoch 2/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.9277 - val_loss: 0.9018\n",
      "Epoch 3/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.8490 - val_loss: 1.0442\n",
      "Epoch 4/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.8102 - val_loss: 1.0682\n",
      "Epoch 5/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.7742 - val_loss: 1.0450\n",
      "Epoch 6/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.7386 - val_loss: 1.0335\n",
      "Epoch 7/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.7198 - val_loss: 1.1188\n",
      "Epoch 8/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.6840 - val_loss: 0.7971\n",
      "Epoch 9/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.6684 - val_loss: 0.8121\n",
      "Epoch 10/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.6388 - val_loss: 0.8124\n",
      "Epoch 11/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.6391 - val_loss: 0.7705\n",
      "Epoch 12/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.6051 - val_loss: 0.8022\n",
      "Epoch 13/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.5991 - val_loss: 0.8838\n",
      "Epoch 14/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.5700 - val_loss: 0.8382\n",
      "Epoch 15/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.5802 - val_loss: 0.7488\n",
      "Epoch 16/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.5438 - val_loss: 0.7179\n",
      "Epoch 17/200\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 0.5528 - val_loss: 0.8096\n",
      "Epoch 18/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.5233 - val_loss: 0.6803\n",
      "Epoch 19/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.5237 - val_loss: 0.7049\n",
      "Epoch 20/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.4958 - val_loss: 0.8960\n",
      "Epoch 21/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.4963 - val_loss: 0.6500\n",
      "Epoch 22/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.4727 - val_loss: 0.7121\n",
      "Epoch 23/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.4655 - val_loss: 0.8297\n",
      "Epoch 24/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.4635 - val_loss: 0.8069\n",
      "Epoch 25/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.4411 - val_loss: 0.7239\n",
      "Epoch 26/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.4476 - val_loss: 0.7325\n",
      "Epoch 27/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.4418 - val_loss: 0.6804\n",
      "Epoch 28/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.4265 - val_loss: 0.6991\n",
      "Epoch 29/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.4013 - val_loss: 0.6282\n",
      "Epoch 30/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.4194 - val_loss: 0.7836\n",
      "Epoch 31/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.3933 - val_loss: 0.6484\n",
      "Epoch 32/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.4083 - val_loss: 0.7870\n",
      "Epoch 33/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.3741 - val_loss: 0.7742\n",
      "Epoch 34/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.3852 - val_loss: 0.7237\n",
      "Epoch 35/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.3572 - val_loss: 0.6469\n",
      "Epoch 36/200\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 0.3633 - val_loss: 0.6375\n",
      "Epoch 37/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.3521 - val_loss: 0.6120\n",
      "Epoch 38/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.3551 - val_loss: 0.7481\n",
      "Epoch 39/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.3399 - val_loss: 0.6549\n",
      "Epoch 40/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.3353 - val_loss: 0.6216\n",
      "Epoch 41/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.3298 - val_loss: 0.6294\n",
      "Epoch 42/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.3216 - val_loss: 0.6119\n",
      "Epoch 43/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.3256 - val_loss: 0.6696\n",
      "Epoch 44/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.3135 - val_loss: 0.6562\n",
      "Epoch 45/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.3126 - val_loss: 0.6637\n",
      "Epoch 46/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.2954 - val_loss: 0.6610\n",
      "Epoch 47/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.3024 - val_loss: 0.6819\n",
      "Epoch 48/200\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.2812 - val_loss: 0.6213\n",
      "Epoch 49/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.2958 - val_loss: 0.6483\n",
      "Epoch 50/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.2748 - val_loss: 0.5898\n",
      "Epoch 51/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.2852 - val_loss: 0.6334\n",
      "Epoch 52/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.2678 - val_loss: 0.7418\n",
      "Epoch 53/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.2706 - val_loss: 0.7181\n",
      "Epoch 54/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.2599 - val_loss: 0.6121\n",
      "Epoch 55/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.2559 - val_loss: 0.6112\n",
      "Epoch 56/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.2498 - val_loss: 0.6826\n",
      "Epoch 57/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.2447 - val_loss: 0.6648\n",
      "Epoch 58/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.2410 - val_loss: 0.6556\n",
      "Epoch 59/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.2311 - val_loss: 0.7789\n",
      "Epoch 60/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.2350 - val_loss: 0.6969\n",
      "Epoch 61/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.2243 - val_loss: 0.6388\n",
      "Epoch 62/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.2252 - val_loss: 0.5870\n",
      "Epoch 63/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.2153 - val_loss: 0.5876\n",
      "Epoch 64/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.2212 - val_loss: 0.5989\n",
      "Epoch 65/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.2065 - val_loss: 0.6050\n",
      "Epoch 66/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.2115 - val_loss: 0.5771\n",
      "Epoch 67/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.2003 - val_loss: 0.5836\n",
      "Epoch 68/200\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 0.2076 - val_loss: 0.5934\n",
      "Epoch 69/200\n",
      "300/300 [==============================] - 5s 18ms/step - loss: 0.1967 - val_loss: 0.5932\n",
      "Epoch 70/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.1924 - val_loss: 0.6387\n",
      "Epoch 71/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.1905 - val_loss: 0.5888\n",
      "Epoch 72/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.1863 - val_loss: 0.5715\n",
      "Epoch 73/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.1815 - val_loss: 0.6559\n",
      "Epoch 74/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1775 - val_loss: 0.5660\n",
      "Epoch 75/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.1845 - val_loss: 0.5840\n",
      "Epoch 76/200\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 0.1712 - val_loss: 0.5813\n",
      "Epoch 77/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.1800 - val_loss: 0.6414\n",
      "Epoch 78/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1674 - val_loss: 0.5826\n",
      "Epoch 79/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1731 - val_loss: 0.5963\n",
      "Epoch 80/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.1611 - val_loss: 0.5940\n",
      "Epoch 81/200\n",
      "300/300 [==============================] - 10s 34ms/step - loss: 0.1626 - val_loss: 0.5957\n",
      "Epoch 82/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.1570 - val_loss: 0.6111\n",
      "Epoch 83/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1598 - val_loss: 0.5969\n",
      "Epoch 84/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.1523 - val_loss: 0.5827\n",
      "Epoch 85/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1513 - val_loss: 0.5837\n",
      "Epoch 86/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.1522 - val_loss: 0.6646\n",
      "Epoch 87/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.1465 - val_loss: 0.6039\n",
      "Epoch 88/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.1498 - val_loss: 0.6313\n",
      "Epoch 89/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.1397 - val_loss: 0.6036\n",
      "Epoch 90/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.1473 - val_loss: 0.5679\n",
      "Epoch 91/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1378 - val_loss: 0.5799\n",
      "Epoch 92/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.1391 - val_loss: 0.5900\n",
      "Epoch 93/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.1357 - val_loss: 0.5724\n",
      "Epoch 94/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.1413 - val_loss: 0.6093\n",
      "Epoch 95/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.1297 - val_loss: 0.5836\n",
      "Epoch 96/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.1347 - val_loss: 0.6051\n",
      "Epoch 97/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.1305 - val_loss: 0.6377\n",
      "Epoch 98/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1309 - val_loss: 0.5644\n",
      "Epoch 99/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.1307 - val_loss: 0.5873\n",
      "Epoch 100/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.1278 - val_loss: 0.5866\n",
      "Epoch 101/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.1244 - val_loss: 0.5605\n",
      "Epoch 102/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.1234 - val_loss: 0.5780\n",
      "Epoch 103/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.1236 - val_loss: 0.5716\n",
      "Epoch 104/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1207 - val_loss: 0.5777\n",
      "Epoch 105/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.1216 - val_loss: 0.5694\n",
      "Epoch 106/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1185 - val_loss: 0.5712\n",
      "Epoch 107/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.1154 - val_loss: 0.5734\n",
      "Epoch 108/200\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.1157 - val_loss: 0.5566\n",
      "Epoch 109/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.1183 - val_loss: 0.5796\n",
      "Epoch 110/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1089 - val_loss: 0.5601\n",
      "Epoch 111/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.1136 - val_loss: 0.5833\n",
      "Epoch 112/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.1100 - val_loss: 0.5803\n",
      "Epoch 113/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1096 - val_loss: 0.5686\n",
      "Epoch 114/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.1058 - val_loss: 0.5882\n",
      "Epoch 115/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.1090 - val_loss: 0.5601\n",
      "Epoch 116/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.1060 - val_loss: 0.5846\n",
      "Epoch 117/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.1074 - val_loss: 0.5756\n",
      "Epoch 118/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.1045 - val_loss: 0.5724\n",
      "Epoch 119/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1072 - val_loss: 0.5659\n",
      "Epoch 120/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.1055 - val_loss: 0.5866\n",
      "Epoch 121/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0998 - val_loss: 0.5699\n",
      "Epoch 122/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.1029 - val_loss: 0.5524\n",
      "Epoch 123/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.1010 - val_loss: 0.5766\n",
      "Epoch 124/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.1014 - val_loss: 0.5692\n",
      "Epoch 125/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.0939 - val_loss: 0.5553\n",
      "Epoch 126/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0998 - val_loss: 0.5815\n",
      "Epoch 127/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0937 - val_loss: 0.5612\n",
      "Epoch 128/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.0971 - val_loss: 0.5656\n",
      "Epoch 129/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0954 - val_loss: 0.5718\n",
      "Epoch 130/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0970 - val_loss: 0.5786\n",
      "Epoch 131/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0920 - val_loss: 0.5570\n",
      "Epoch 132/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0922 - val_loss: 0.5795\n",
      "Epoch 133/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.0899 - val_loss: 0.5690\n",
      "Epoch 134/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0905 - val_loss: 0.5939\n",
      "Epoch 135/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0877 - val_loss: 0.5889\n",
      "Epoch 136/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0901 - val_loss: 0.5708\n",
      "Epoch 137/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0891 - val_loss: 0.5636\n",
      "Epoch 138/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.0862 - val_loss: 0.5697\n",
      "Epoch 139/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.0883 - val_loss: 0.5777\n",
      "Epoch 140/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.0852 - val_loss: 0.5796\n",
      "Epoch 141/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0879 - val_loss: 0.5681\n",
      "Epoch 142/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0849 - val_loss: 0.5784\n",
      "Epoch 143/200\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 0.0869 - val_loss: 0.5788\n",
      "Epoch 144/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0845 - val_loss: 0.5749\n",
      "Epoch 145/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0843 - val_loss: 0.5678\n",
      "Epoch 146/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0829 - val_loss: 0.5711\n",
      "Epoch 147/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.0839 - val_loss: 0.5670\n",
      "Epoch 148/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0827 - val_loss: 0.5705\n",
      "Epoch 149/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0806 - val_loss: 0.5690\n",
      "Epoch 150/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0803 - val_loss: 0.5704\n",
      "Epoch 151/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0793 - val_loss: 0.5702\n",
      "Epoch 152/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.0816 - val_loss: 0.5891\n",
      "Epoch 153/200\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 0.0778 - val_loss: 0.5685\n",
      "Epoch 154/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0786 - val_loss: 0.5812\n",
      "Epoch 155/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0780 - val_loss: 0.5738\n",
      "Epoch 156/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0790 - val_loss: 0.5749\n",
      "Epoch 157/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0755 - val_loss: 0.5773\n",
      "Epoch 158/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0792 - val_loss: 0.5693\n",
      "Epoch 159/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0758 - val_loss: 0.5627\n",
      "Epoch 160/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0743 - val_loss: 0.5738\n",
      "Epoch 161/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.0744 - val_loss: 0.5690\n",
      "Epoch 162/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.0767 - val_loss: 0.5642\n",
      "Epoch 163/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0746 - val_loss: 0.5883\n",
      "Epoch 164/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0746 - val_loss: 0.5611\n",
      "Epoch 165/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.0715 - val_loss: 0.5685\n",
      "Epoch 166/200\n",
      "300/300 [==============================] - 10s 34ms/step - loss: 0.0732 - val_loss: 0.5675\n",
      "Epoch 167/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.0729 - val_loss: 0.5650\n",
      "Epoch 168/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0719 - val_loss: 0.5867\n",
      "Epoch 169/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0751 - val_loss: 0.5745\n",
      "Epoch 170/200\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 0.0720 - val_loss: 0.5709\n",
      "Epoch 171/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.0705 - val_loss: 0.5730\n",
      "Epoch 172/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0695 - val_loss: 0.5762\n",
      "Epoch 173/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.0709 - val_loss: 0.5774\n",
      "Epoch 174/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.0673 - val_loss: 0.5869\n",
      "Epoch 175/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.0705 - val_loss: 0.5896\n",
      "Epoch 176/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0688 - val_loss: 0.5802\n",
      "Epoch 177/200\n",
      "300/300 [==============================] - 9s 30ms/step - loss: 0.0684 - val_loss: 0.5726\n",
      "Epoch 178/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0650 - val_loss: 0.5728\n",
      "Epoch 179/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0684 - val_loss: 0.5684\n",
      "Epoch 180/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.0661 - val_loss: 0.5843\n",
      "Epoch 181/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0680 - val_loss: 0.5765\n",
      "Epoch 182/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0653 - val_loss: 0.5710\n",
      "Epoch 183/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0649 - val_loss: 0.5730\n",
      "Epoch 184/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0647 - val_loss: 0.5811\n",
      "Epoch 185/200\n",
      "300/300 [==============================] - 10s 33ms/step - loss: 0.0634 - val_loss: 0.5722\n",
      "Epoch 186/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.0646 - val_loss: 0.5931\n",
      "Epoch 187/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0620 - val_loss: 0.5668\n",
      "Epoch 188/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.0647 - val_loss: 0.5728\n",
      "Epoch 189/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0625 - val_loss: 0.5750\n",
      "Epoch 190/200\n",
      "300/300 [==============================] - 9s 31ms/step - loss: 0.0649 - val_loss: 0.5829\n",
      "Epoch 191/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.0629 - val_loss: 0.5751\n",
      "Epoch 192/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0621 - val_loss: 0.5738\n",
      "Epoch 193/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.0629 - val_loss: 0.5723\n",
      "Epoch 194/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0619 - val_loss: 0.5817\n",
      "Epoch 195/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0626 - val_loss: 0.5837\n",
      "Epoch 196/200\n",
      "300/300 [==============================] - 9s 32ms/step - loss: 0.0609 - val_loss: 0.5977\n",
      "Epoch 197/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.0596 - val_loss: 0.5931\n",
      "Epoch 198/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0611 - val_loss: 0.6056\n",
      "Epoch 199/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.0612 - val_loss: 0.5741\n",
      "Epoch 200/200\n",
      "300/300 [==============================] - 10s 32ms/step - loss: 0.0603 - val_loss: 0.5812\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <h2> model - RMSE report </h2>\n",
       "    <h3> RMSE </h3>\n",
       "    <hr>\n",
       "    <div>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td>RMSE (baseline &forall;1.0)</td>\n",
       "                <td>1.53</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>RMSE</td>\n",
       "                <td>0.766</td>\n",
       "            </tr>\n",
       "        </table>\n",
       "    <hr>\n",
       "    <h3> Partial RMSE </h3>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td>Mean partial RMSE (baseline &forall;1.0)</td>\n",
       "                <td>2.0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>Max partial RMSE (baseline &forall;1.0)</td>\n",
       "                <td>1.414</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>St.dev. partial RMSE (baseline &forall;1.0)</td>\n",
       "                <td>4.0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>Mean partial RMSE</td>\n",
       "                <td><b>0.927</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>Max partial RMSE</td>\n",
       "                <td>0.212</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>St.dev. partial RMSE</td>\n",
       "                <td>1.189</td>\n",
       "            </tr>            \n",
       "        </table>\n",
       "    </div>\n",
       "    <h3> Improvement over baseline (&forall;1.0) </h3>\n",
       "    <hr>\n",
       "    <div>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td>RMSE</td>\n",
       "                <td>0.764</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>Mean partial RMSE</td>\n",
       "                <td><b>1.073</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>Max partial RMSE</td>\n",
       "                <td>2.811</td>\n",
       "            </tr>\n",
       "        </table>\n",
       "    </div>\n",
       "    \n",
       "    <h3> Partial RMSE detailed</h3>\n",
       "    <hr>\n",
       "    <div>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <th>Review Score</th>\n",
       "                <th>RMSE</th>\n",
       "                <th>RMSE baseline (&forall;1.0)</th>\n",
       "                <th>Improvement over baseline</th>\n",
       "            </tr>\n",
       "            \n",
       "    <tr>\n",
       "        <td>\n",
       "            5.0\n",
       "        </td>\n",
       "        <td>\n",
       "            0.619\n",
       "        </td>\n",
       "        <td>\n",
       "            0.0\n",
       "        </td>\n",
       "        <td>\n",
       "            -0.619\n",
       "        </td>\n",
       "    </tr>\n",
       "    \n",
       "\n",
       "    <tr>\n",
       "        <td>\n",
       "            4.0\n",
       "        </td>\n",
       "        <td>\n",
       "            0.759\n",
       "        </td>\n",
       "        <td>\n",
       "            1.0\n",
       "        </td>\n",
       "        <td>\n",
       "            0.241\n",
       "        </td>\n",
       "    </tr>\n",
       "    \n",
       "\n",
       "    <tr>\n",
       "        <td>\n",
       "            3.0\n",
       "        </td>\n",
       "        <td>\n",
       "            0.968\n",
       "        </td>\n",
       "        <td>\n",
       "            2.0\n",
       "        </td>\n",
       "        <td>\n",
       "            1.032\n",
       "        </td>\n",
       "    </tr>\n",
       "    \n",
       "\n",
       "    <tr>\n",
       "        <td>\n",
       "            2.0\n",
       "        </td>\n",
       "        <td>\n",
       "            1.102\n",
       "        </td>\n",
       "        <td>\n",
       "            3.0\n",
       "        </td>\n",
       "        <td>\n",
       "            1.898\n",
       "        </td>\n",
       "    </tr>\n",
       "    \n",
       "\n",
       "    <tr>\n",
       "        <td>\n",
       "            1.0\n",
       "        </td>\n",
       "        <td>\n",
       "            1.189\n",
       "        </td>\n",
       "        <td>\n",
       "            4.0\n",
       "        </td>\n",
       "        <td>\n",
       "            2.811\n",
       "        </td>\n",
       "    </tr>\n",
       "    \n",
       "        </table>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=experiment(learning_rate=0.04,epochs=200,batch_size=256,steps=300,name='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 25, 256)           256256    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 322,305\n",
      "Trainable params: 322,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Persist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
