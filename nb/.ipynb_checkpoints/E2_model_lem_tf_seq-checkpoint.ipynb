{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip imbalanced-learn\n",
    "#!pip install tensorflow-gpu==2.0.0-rc0 \n",
    "\n",
    "#tensorboard --logdir /home/kvassay/data/z/log/E2/scalars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "import os\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import tensorflow.keras as K\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize as scikit_normalize\n",
    "from sklearn.utils import shuffle as  sklearn_shuffle\n",
    "\n",
    "from IPython.core.display import display\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET='/home/kvassay/data/z/data/reviews_train_test_dev1_{}.pickle'\n",
    "TYPE='lem_tok'\n",
    "TB_LOG_DIR='/home/kvassay/data/z/log/E2/scalars/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.42 s, sys: 943 ms, total: 5.36 s\n",
      "Wall time: 5.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(DATASET.format(TYPE),'rb') as f:\n",
    "    train,dev,test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_train(dataset,key, **scikit_kwargs):\n",
    "    vectorizer=TfidfVectorizer(**scikit_kwargs)\n",
    "    vectorizer.fit([' '.join(x[key]) for x in dataset])\n",
    "    return vectorizer\n",
    "\n",
    "def tf_predict(vectorizer,dataset,key, apply_norm=True):\n",
    "    features=vectorizer.transform([' '.join(x[key]) for x in dataset])\n",
    "    if apply_norm is True:\n",
    "        features=scikit_normalize(features)\n",
    "    return features\n",
    "\n",
    "def extract_features(dataset,vectorizer_summary,vectorizer_text):\n",
    "    summ_vecs=tf_predict(vectorizer_summary,dataset,'summary')\n",
    "    text_vecs=tf_predict(vectorizer_text,dataset,'text')\n",
    "    return hstack([summ_vecs, text_vecs],format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 1.5 s, total: 1min 12s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer_text=tf_train(train,'text',max_features=35000,ngram_range=(1,2),max_df=0.99,lowercase=True,use_idf=False)\n",
    "vectorizer_summary=tf_train(train,'summary',max_features=5000,ngram_range=(1,2),max_df=0.99,lowercase=True,use_idf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples shape: (551399, 40000), Dev samples shape: (8527, 40000)\n",
      "CPU times: user 1min, sys: 1.56 s, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train=extract_features(train, vectorizer_summary,vectorizer_text)\n",
    "X_dev=extract_features(dev, vectorizer_summary,vectorizer_text)\n",
    "y_train=np.array([x['score'] for x in train])\n",
    "y_dev=np.array([x['score'] for x in dev])\n",
    "print('Train samples shape: {}, Dev samples shape: {}'.format(X_train.shape,X_dev.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnderSampler(K.utils.Sequence):\n",
    "\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.rus=RandomUnderSampler(sampling_strategy='not minority')\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size = batch_size\n",
    "        #len\n",
    "        self._shuffle()\n",
    "        self.length= math.ceil(self.X_u.shape[0] / self.batch_size)\n",
    "\n",
    "    def _shuffle(self):\n",
    "        self.X_u, self.y_u=self.rus.fit_resample(self.X,self.y)\n",
    "        self.X_u,self.y_u = sklearn_shuffle(self.X_u,self.y_u)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.X_u[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y_u[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x.todense(), batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self._shuffle()\n",
    "        \n",
    "class RandomSampler(K.utils.Sequence):\n",
    "\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size = batch_size\n",
    "        self.sampling_size=np.min([x for (_,x) in dict(Counter(y)).items()])*len(set(y))\n",
    "        #len\n",
    "        self._shuffle()\n",
    "        self.length= math.ceil(self.X_u.shape[0] / self.batch_size)\n",
    "\n",
    "    def _shuffle(self):\n",
    "        self.X_u,_,self.y_u,_=train_test_split(self.X,self.y,shuffle=True,train_size=self.sampling_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.X_u[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y_u[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x.todense(),batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self._shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tb_callback():\n",
    "    suffix=datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir= os.path.join(TB_LOG_DIR,suffix)\n",
    "    return K.callbacks.TensorBoard(log_dir=os.path.join(log_dir))\n",
    "\n",
    "def train_model(batch_size,learning_rate, epochs, sampler_cls=UnderSampler):\n",
    "    tensorboard_callback = get_tb_callback()\n",
    "    model = K.models.Sequential([\n",
    "        K.layers.Dense(100,activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        K.layers.Dropout(0.2),\n",
    "        K.layers.Dense(20,activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        K.layers.Dense(1,activation='linear'),\n",
    "    ])\n",
    "    opt=K.optimizers.Adam(lr=learning_rate, decay=learning_rate/epochs, amsgrad=True)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    sampler=sampler_cls(X_train,y_train,batch_size=batch_size)\n",
    "    model.fit_generator(sampler,\n",
    "                        shuffle=False,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(X_dev.todense(),y_dev),\n",
    "                        callbacks=[tensorboard_callback])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model,title):\n",
    "    loss = model.history.history['loss']\n",
    "    val_loss = model.history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, color='red', label='MSE Train')\n",
    "    plt.plot(epochs, val_loss, color='green', label='MSE Dev')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs') \n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def rmse(y_true,y_pred):\n",
    "    return sqrt(mean_squared_error(y_true,y_pred))\n",
    "\n",
    "def rmse_partial(y_true, y_pred):\n",
    "    all_scores=list(set(y_true))\n",
    "    y_true_dict={x:[] for x in all_scores}\n",
    "    y_pred_dict={x:[] for x in all_scores}\n",
    "    for i, score in enumerate(y_true):\n",
    "        y_true_dict[score].append(score)\n",
    "        y_pred_dict[score].append(y_pred[i])\n",
    "    return {score: rmse(y_true_dict[score],y_pred_dict[score]) for score in all_scores}\n",
    "        \n",
    "def rmse_partial_avg(y_true, y_pred):\n",
    "    rmse_dict=rmse_partial(y_true,y_pred)\n",
    "    values=list(rmse_dict.values())\n",
    "    return np.mean(values), np.std(values)\n",
    "\n",
    "def rmse_partial_max(y_true, y_pred):\n",
    "    rmse_dict=rmse_partial(y_true,y_pred)\n",
    "    values=list(rmse_dict.values())\n",
    "    return np.max(values)\n",
    "\n",
    "def rmse_report(y_true,y_pred,round_decimals=3, title='RMSE report'):\n",
    "    def ar(x):\n",
    "        return np.around(x,decimals=round_decimals)\n",
    "    baseline=rmse(y_true,[5.0]*len(y_true))\n",
    "    HTML_TEMPLATE=\"\"\"\n",
    "    <h2> {} </h2>\n",
    "    <h3> RMSE </h3>\n",
    "    <hr>\n",
    "    <div>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <td>RMSE (baseline &forall;1.0)</td>\n",
    "                <td>{}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>RMSE</td>\n",
    "                <td>{}</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    <hr>\n",
    "    <h3> Partial RMSE </h3>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <td>Mean partial RMSE (baseline &forall;1.0)</td>\n",
    "                <td>{}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Max partial RMSE (baseline &forall;1.0)</td>\n",
    "                <td>{}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>St.dev. partial RMSE (baseline &forall;1.0)</td>\n",
    "                <td>{}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Mean partial RMSE</td>\n",
    "                <td><b>{}</b></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Max partial RMSE</td>\n",
    "                <td>{}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>St.dev. partial RMSE</td>\n",
    "                <td>{}</td>\n",
    "            </tr>            \n",
    "        </table>\n",
    "    </div>\n",
    "    <h3> Improvement over baseline (&forall;1.0) </h3>\n",
    "    <hr>\n",
    "    <div>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <td>RMSE</td>\n",
    "                <td>{}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Mean partial RMSE</td>\n",
    "                <td><b>{}</b></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Max partial RMSE</td>\n",
    "                <td>{}</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "    \n",
    "    <h3> Partial RMSE detailed</h3>\n",
    "    <hr>\n",
    "    <div>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Review Score</th>\n",
    "                <th>RMSE</th>\n",
    "                <th>RMSE baseline (&forall;1.0)</th>\n",
    "                <th>Improvement over baseline</th>\n",
    "            </tr>\n",
    "            {}\n",
    "        </table>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    PARTIAL_ROW_TEMPLATE='''\n",
    "    <tr>\n",
    "        <td>\n",
    "            {}\n",
    "        </td>\n",
    "        <td>\n",
    "            {}\n",
    "        </td>\n",
    "        <td>\n",
    "            {}\n",
    "        </td>\n",
    "        <td>\n",
    "            {}\n",
    "        </td>\n",
    "    </tr>\n",
    "    '''\n",
    "    overall=rmse(y_true,y_pred)\n",
    "    partial_avg,partial_std=rmse_partial_avg(y_true,y_pred)\n",
    "    partial_max=rmse_partial_max(y_true,y_pred)\n",
    "    partial_avg_baseline,partial_std_baseline=rmse_partial_avg(y_true,[5.0]*len(y_true))\n",
    "    partial_max_baseline=rmse_partial_max(y_true,[5.0]*len(y_true))\n",
    "    improvement_marmse=partial_avg_baseline-partial_avg\n",
    "    improvement_rmse=baseline-overall\n",
    "    improvement_rmse_partial_max=partial_max_baseline - partial_max\n",
    "    \n",
    "    #partial rows\n",
    "    partial=rmse_partial(y_true,y_pred)\n",
    "    partial_baseline=rmse_partial(y_true, [5.0]*len(y_true))\n",
    "    partial=sorted(partial.items(),key=lambda x:x[0],reverse=True)\n",
    "    partial_table_rows=[]\n",
    "    for key,value in partial:\n",
    "        value_baseline=partial_baseline[key]\n",
    "        diff_baseline=value_baseline-value\n",
    "        partial_table_rows.append(PARTIAL_ROW_TEMPLATE.format(key,ar(value),ar(value_baseline),ar(diff_baseline)))\n",
    "    partial_table_rows='\\n'.join(partial_table_rows)\n",
    "    \n",
    "    html=HTML_TEMPLATE.format(title,\n",
    "                              ar(baseline),\n",
    "                              ar(overall),\n",
    "                              ar(partial_avg_baseline),\n",
    "                              ar(partial_std_baseline),\n",
    "                              ar(partial_max_baseline),\n",
    "                              ar(partial_avg),\n",
    "                              ar(partial_std),\n",
    "                              ar(partial_max),\n",
    "                              ar(improvement_rmse),\n",
    "                              ar(improvement_marmse),\n",
    "                              ar(improvement_rmse_partial_max),\n",
    "                              partial_table_rows)\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(sampling_cls,learning_rate,epochs,batch_size,name):\n",
    "    model=train_model(sampler_cls=sampling_cls,epochs=epochs,batch_size=batch_size,learning_rate=learning_rate)\n",
    "    y_pred_dev=model.predict(X_dev)\n",
    "    rmse_report(y_dev,y_pred_dev,title='{} - RMSE report'.format(name))\n",
    "    plot_history(model,title='{} - Train/Dev MSE'.format(name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "482/482 [==============================] - 98s 203ms/step - loss: 0.8144 - mean_squared_error: 0.8150 - val_loss: 0.6039 - val_mean_squared_error: 0.6036\n",
      "Epoch 2/25\n",
      "482/482 [==============================] - 97s 201ms/step - loss: 0.4659 - mean_squared_error: 0.4660 - val_loss: 0.4768 - val_mean_squared_error: 0.4768\n",
      "Epoch 3/25\n",
      "482/482 [==============================] - 97s 201ms/step - loss: 0.3689 - mean_squared_error: 0.3689 - val_loss: 0.4508 - val_mean_squared_error: 0.4510\n",
      "Epoch 4/25\n",
      "482/482 [==============================] - 97s 200ms/step - loss: 0.3163 - mean_squared_error: 0.3165 - val_loss: 0.4601 - val_mean_squared_error: 0.4603\n",
      "Epoch 5/25\n",
      "466/482 [============================>.] - ETA: 3s - loss: 0.2759 - mean_squared_error: 0.2759"
     ]
    }
   ],
   "source": [
    "model=experiment(sampling_cls=UnderSampler,learning_rate=0.05, epochs=25,batch_size=300,name='TF-IDF model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/kvassay/data/z/models/E2/vectorizer_summary.pickle','wb') as f:\n",
    "    pickle.dump(vectorizer_summary,f)\n",
    "with open('/home/kvassay/data/z/models/E2/vectorizer_text.pickle','wb') as f:\n",
    "    pickle.dump(vectorizer_text,f)\n",
    "model.save('/home/kvassay/data/z/models/E2/keras_regressor.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
