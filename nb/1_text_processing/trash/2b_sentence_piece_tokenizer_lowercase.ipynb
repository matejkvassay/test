{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sentencepiece as spm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE='/home/kvassay/data/z/models/sentencepiece/sp_lc_{}' \n",
    "DATASET='/home/kvassay/data/z/data/reviews_train_test_dev1.pickle'\n",
    "TMP_DATASET_FILE='/tmp/spt_train.txt'\n",
    "VOCAB_SIZES=[100,500,700,1000,2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET,'rb') as f:\n",
    "    train,dev,_=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_file(dataset, save_path, limit_data=None):\n",
    "    with open(TMP_DATASET_FILE, 'w') as f:\n",
    "        if limit_data is None:\n",
    "            limit=10000000000\n",
    "        else:\n",
    "            limit=limit_data\n",
    "        f.writelines([x['text'].lower()+'\\n' for x in dataset if x][:limit])\n",
    "        \n",
    "def train_sentpiece(dataset_path, vocab_size, save_path):\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input={} --model_prefix={} --vocab_size={}'.format(\n",
    "            dataset_path,\n",
    "            save_path,\n",
    "            vocab_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_training_file(train,TMP_DATASET_FILE, limit_data=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 100 tokens model took 179.31135082244873 seconds.\n",
      "Training 500 tokens model took 173.569965839386 seconds.\n",
      "Training 700 tokens model took 172.5167191028595 seconds.\n",
      "Training 1000 tokens model took 172.21199893951416 seconds.\n",
      "Training 2000 tokens model took 169.3970229625702 seconds.\n"
     ]
    }
   ],
   "source": [
    "for vocab_size in VOCAB_SIZES:\n",
    "    start=time()\n",
    "    train_sentpiece(TMP_DATASET_FILE, vocab_size, MODEL_SAVE.format(vocab_size))\n",
    "    end=time()\n",
    "    print('Training {} tokens model took {} seconds.'.format(vocab_size,end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model='/home/kvassay/data/z/models/sentencepiece/sp_lc_2000.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID=1951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My dog eats Nature\\'s Recipe \"Easy to Digest\" dry food everyday. It really helped with her digestive issues. I thought the wet food version would be okay, but it made me completely ill. She was dry-heaving, had major gas issues and was quite lethargic. About 12 hours after her final dose of it, all of her symptoms disappeared. The dry food is still great, but this wet food is really awful.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[ID]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁my',\n",
       " '▁dog',\n",
       " '▁eat',\n",
       " 's',\n",
       " '▁nature',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁recipe',\n",
       " '▁\"',\n",
       " 'ea',\n",
       " 's',\n",
       " 'y',\n",
       " '▁to',\n",
       " '▁digest',\n",
       " '\"',\n",
       " '▁dry',\n",
       " '▁food',\n",
       " '▁everyday',\n",
       " '.',\n",
       " '▁it',\n",
       " '▁really',\n",
       " '▁help',\n",
       " 'ed',\n",
       " '▁with',\n",
       " '▁her',\n",
       " '▁digest',\n",
       " 'ive',\n",
       " '▁issues',\n",
       " '.',\n",
       " '▁i',\n",
       " '▁thought',\n",
       " '▁the',\n",
       " '▁we',\n",
       " 't',\n",
       " '▁food',\n",
       " '▁version',\n",
       " '▁would',\n",
       " '▁be',\n",
       " '▁okay',\n",
       " ',',\n",
       " '▁but',\n",
       " '▁it',\n",
       " '▁made',\n",
       " '▁me',\n",
       " '▁completely',\n",
       " '▁i',\n",
       " 'll',\n",
       " '.',\n",
       " '▁she',\n",
       " '▁was',\n",
       " '▁dry',\n",
       " '-',\n",
       " 'he',\n",
       " 'a',\n",
       " 'v',\n",
       " 'ing',\n",
       " ',',\n",
       " '▁had',\n",
       " '▁major',\n",
       " '▁ga',\n",
       " 's',\n",
       " '▁issues',\n",
       " '▁and',\n",
       " '▁was',\n",
       " '▁quite',\n",
       " '▁let',\n",
       " 'h',\n",
       " 'ar',\n",
       " 'g',\n",
       " 'ic',\n",
       " '.',\n",
       " '▁about',\n",
       " '▁12',\n",
       " '▁hours',\n",
       " '▁after',\n",
       " '▁her',\n",
       " '▁fi',\n",
       " 'n',\n",
       " 'al',\n",
       " '▁do',\n",
       " 'se',\n",
       " '▁of',\n",
       " '▁it',\n",
       " ',',\n",
       " '▁all',\n",
       " '▁of',\n",
       " '▁her',\n",
       " '▁',\n",
       " 's',\n",
       " 'y',\n",
       " 'mp',\n",
       " 'to',\n",
       " 'm',\n",
       " 's',\n",
       " '▁dis',\n",
       " 'a',\n",
       " 'pp',\n",
       " 'ear',\n",
       " 'ed',\n",
       " '.',\n",
       " '▁the',\n",
       " '▁dry',\n",
       " '▁food',\n",
       " '▁is',\n",
       " '▁still',\n",
       " '▁great',\n",
       " ',',\n",
       " '▁but',\n",
       " '▁this',\n",
       " '▁we',\n",
       " 't',\n",
       " '▁food',\n",
       " '▁is',\n",
       " '▁really',\n",
       " '▁awful',\n",
       " '.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.EncodeAsPieces(dev[ID]['text'].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My dog eats Nature\\'s Recipe \"Easy to Digest\" dry food everyday. It really helped with her digestive issues. I thought the wet food version would be okay, but it made me completely ill. She was dry-heaving, had major gas issues and was quite lethargic. About 12 hours after her final dose of it, all of her symptoms disappeared. The dry food is still great, but this wet food is really awful.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.DecodePieces(tokenizer.EncodeAsPieces(dev[ID]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
